{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 - Probability, Linear Algebra, & Computational Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 2: Updated 1/14/24 (Question 4 revised)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *BARBARA FLORES RIOS*\n",
    "Netid: bpf17\n",
    "\n",
    "Note: this assignment falls under collaboration Mode 2: Individual Assignment â€“ Collaboration Permitted. Please refer to the syllabus for additional information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions for all assignments can be found [here](https://kylebradbury.github.io/ids705/notebooks/assignment_instructions.html), and are also linked to from the course syllabus.\n",
    "\n",
    "Total points in the assignment add up to 90; an additional 10 points are allocated to professionalism and presentation quality."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Learning Objectives\n",
    "The purpose of this assignment is to provide a refresher on fundamental concepts that we will use throughout this course and provide an opportunity to develop skills in any of the related skills that may be unfamiliar to you. Through the course of completing this assignment, you will... \n",
    "\n",
    "- Refresh you knowledge of probability theory including properties of random variables, probability density functions,  cumulative distribution functions, and key statistics such as mean and variance.\n",
    "- Revisit common linear algebra and matrix operations and concepts such as matrix multiplication, inner and outer products, inverses, the Hadamard (element-wise) product, eigenvalues and eigenvectors, orthogonality, and symmetry.\n",
    "- Practice numerical programming, core to machine learning, by loading and filtering data, plotting data, vectorizing operations, profiling code speed, and debugging and optimizing performance. You will also practice computing probabilities based on simulation.\n",
    "- Develop or refresh your knowledge of Git version control, which will be a core tool used in the final project of this course\n",
    "- Apply your skills altogether through an exploratory data analysis to practice data cleaning, data manipulation, interpretation, and communication \n",
    "\n",
    "We will build on these concepts throughout the course, so use this assignment as a catalyst to deepen your knowledge and seek help with anything unfamiliar.\n",
    "\n",
    "If some references would be helpful on these topics, I would recommend the following resources:\n",
    "- [Mathematics for Machine Learning](https://mml-book.github.io/book/mml-book.pdf) by Deisenroth, Faisal, and Ong\n",
    "- [Deep Learning](https://www.deeplearningbook.org/); Part I: Applied Math and Machine Learning Basics by Goodfellow, Bengio, and Courville\n",
    "- [The Matrix Calculus You Need For Deep Learning](https://arxiv.org/pdf/1802.01528.pdf) by Parr and Howard\n",
    "- [Dive Into Deep Learning](https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/index.html); Appendix: Mathematics for Deep Learning by Weness, Hu, et al.\n",
    "\n",
    "*Note: don't worry if you don't understand everything in the references above - some of these books dive into significant minutia of each of these topics.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Probability and Statistics Theory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: for all assignments, write out equations and math using markdown and [LaTeX](https://tobi.oetiker.ch/lshort/lshort.pdf). I recommend that you complete the work on paper before typing up the final version. For this assignment show your math for questions 1-4, meaning that you should include any intermediate steps necessary to understand the logic of your solution. Most can be completed in 3-4 steps. Being proficient in expressing yourself clearly, sometimes mathematically, is a valuable skill to have as a data scientist*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1\n",
    "**[3 points]**  \n",
    "Let $f(x) = \\begin{cases}\n",
    "                0           & x < 0  \\\\\n",
    "                \\alpha x^2  & 0 \\leq x \\leq 2 \\\\\n",
    "                0           & 2 < x\n",
    "            \\end{cases}$\n",
    "            \n",
    "For what value of $\\alpha$ is $f(x)$ a valid probability density function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**\n",
    "\n",
    ">The area under the curve of the function should be equal to 1 to meet the conditions of a probability density function. \n",
    ">Therefore, we calculate the integral of the function and set it equal to 1.\n",
    ">\n",
    ">The area under the curve of $f(x)$ is 0 when $x < 0$ and when $x > 2$, so we will only consider the interval $0 \\leq x \\leq 2$.\n",
    ">So we calculate the following:\n",
    ">\n",
    "$$\n",
    "\\int_0^2 \\alpha x^2 \\,dx\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2\n",
    "**[3 points]** What is the cumulative distribution function (CDF) that corresponds to the following probability distribution function? Please state the value of the CDF for all possible values of $x$.\n",
    "\n",
    "$f(x) = \\begin{cases}\n",
    "    \\frac{1}{3} & 0 < x < 3 \\\\\n",
    "    0           & \\text{otherwise}\n",
    "    \\end{cases}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3\n",
    "**[6 points]** For the probability distribution function for the random variable $X$,\n",
    "\n",
    "$f(x) = \\begin{cases}\n",
    "    \\frac{1}{3} & 0 < x < 3 \\\\\n",
    "    0           & \\text{otherwise}\n",
    "    \\end{cases}$\n",
    "    \n",
    "what is the (a) expected value and (b) variance of $X$. *Show all work*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4\n",
    "**[6 points]** \n",
    "You are given the training data below and asked to determine the probability that a sample of $x=0.54$ comes from class 1, or equivalently, $P(Y=1|X=0.54)$. The feature, $x$, can take on real values between 0 and 1.\n",
    "\n",
    "| $x$ value range | Negative data samples ($x$,$y=0$) | Positive data samples ($x$,$y=1$) |\n",
    "| -- | -- | -- |\n",
    "| 0.0 - 0.1 | (0.05,0),(0.07,0)|   None   |\n",
    "| 0.1 - 0.2 | (0.11,0),(0.13,0),(0.19,0) | (0.14,1) |\n",
    "| 0.2 - 0.3 | (0.23,0) | (0.24,1) |\n",
    "| 0.3 - 0.4 | (0.35,0), (0.37,0)| (0.32,1) |\n",
    "| 0.4 - 0.5 | (0.49,0) | (0.47,1) |\n",
    "| 0.5 - 0.6 | (0.51,0) | (0.53,1) |\n",
    "| 0.6 - 0.7 |   None   | (0.61,1) |\n",
    "| 0.7 - 0.8 |   None   | (0.77,1) |\n",
    "| 0.8 - 0.9 |   None   | (0.83,1) |\n",
    "| 0.9 - 1.0 |   None   | (0.92,1),(0.98,1) |\n",
    "\n",
    "Note: *You don't need to use these data directly*, but this provides an example of how the data could be distributed to form the empirical likelihoods and priors shown below.\n",
    "\n",
    "You're likely familiar with Bayes' Rule, which for discrete random variables states that:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "P(Y|X)= \\frac{P(X|Y)P(Y)}{P(X)}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, $P(X) = \\sum_y P(X|Y=y)P(Y=y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, however, the variable Y is discrete but the variable X is continuous, so we express this function a bit differently. We can compute the poster probability, $P(Y|X)$, based on the likelihood function of the data conditioned on the class of the samples, $f_{X|Y}(x,y)$, the prior $P(Y)$ which is essentially the distribution of the class labels across all the data, and the evidence $f_X(x)$ which is the probability distribution function of the features, $X$, regardless of class labels:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\\begin{equation*}\n",
    "P(Y=y|X=x)= \\frac{f_{X|Y}(x,y)P(Y=y)}{f_X(x)}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, note that $f_X(x)$ can be computed in this case (due to the discrete Y values of 0 and 1) as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "f_X(x) = f_{X|Y}(x,y=0)P(Y=0) + f_{X|Y}(x,y=1)P(Y=1)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the prior and the likelihood functions based on the dataset above. Note that here we use $P(\\cdot)$ to note a probability and $f(\\cdot)$ to note a probability distribution function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/a1_py.png\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/a1_pxy.png\"  width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is $f_{X|Y}(x=0.54,y=1)P(Y=1)$?\n",
    "2. What is $f_X(x=0.54)$?\n",
    "3. What is $P(Y=1|X=0.54)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Show each value you use for each computation.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Linear Algebra"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5\n",
    "\n",
    "**[5 points]** A common task in machine learning is a change of basis: transforming the representation of our data from one space to another. A prime example of this is through the process of dimensionality reduction as in Principle Components Analysis where we often seek to transform our data from one space (of dimension $n$) to a new space (of dimension $m$) where $m<n$. Assume we have a sample of data of dimension $n=4$ (as shown below) and we want to transform it into a dimension of $m=2$.\n",
    "\n",
    "$\\mathbf{x} =  \\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3 \\\\\n",
    "x_4 \n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) What are the dimensions of a matrix, $\\mathbf{A}$, that would linearly transform our sample of data, $\\mathbf{x}$, into a space of $m=2$ through the operation $\\mathbf{Ax}$?\n",
    "\n",
    "(b) Express this transformation in terms of the components of $\\mathbf{x}$: $x_1$, $x_2$, $x_3$, $x_4$ and the matrix $\\mathbf{A}$ where each entry in the matrix is denoted as $a_{i,j}$ (e.g. the entry in the first row and second column would be $a_{1,2}$). Your answer will be in the form of a matrix expressing result of the product $\\mathbf{Ax}$.\n",
    "\n",
    "*Note: please write your answers here in LaTeX*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6\n",
    "**[14 points]** **Matrix manipulations and multiplication**. Machine learning involves working with many matrices, so this exercise will provide you with the opportunity to practice those skills.\n",
    "\n",
    "Let\n",
    "$\\mathbf{A} =  \\begin{bmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "2 & 4 & 5 \\\\\n",
    "3 & 5 & 6 \n",
    "\\end{bmatrix}$, $\\mathbf{b} =  \\begin{bmatrix}\n",
    "-1  \\\\\n",
    "3  \\\\\n",
    "8  \n",
    "\\end{bmatrix}$, $\\mathbf{c} =  \\begin{bmatrix}\n",
    "4  \\\\\n",
    "-3  \\\\\n",
    "6  \n",
    "\\end{bmatrix}$, and $\\mathbf{I} =  \\begin{bmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1 \n",
    "\\end{bmatrix}$\n",
    "\n",
    "Compute the following **using Python** or indicate that it cannot be computed. Refer to NumPy's tools for handling matrices. While all answers should be computer using Python, your response to whether each item can be computed should refer to underlying linear algebra. There may be circumstances when Python will produce an output, but based on the dimensions of the matrices involved, the linear algebra operation is not possible. **For the case when an operation is invalid, explain why it is not.**\n",
    "\n",
    "When the quantity can be computed, please provide both the Python code AND the output of that code (this need not be in LaTex)\n",
    "\n",
    "1. $\\mathbf{A}\\mathbf{A}$\n",
    "2. $\\mathbf{A}\\mathbf{A}^T$\n",
    "3. $\\mathbf{A}\\mathbf{b}$\n",
    "4. $\\mathbf{A}\\mathbf{b}^T$\n",
    "5. $\\mathbf{b}\\mathbf{A}$\n",
    "6. $\\mathbf{b}^T\\mathbf{A}$\n",
    "7. $\\mathbf{b}\\mathbf{b}$\n",
    "8. $\\mathbf{b}^T\\mathbf{b}$\n",
    "9. $\\mathbf{b}\\mathbf{b}^T$\n",
    "10. $\\mathbf{b} + \\mathbf{c}^T$\n",
    "11. $\\mathbf{b}^T\\mathbf{b}^T$\n",
    "12. $\\mathbf{A}^{-1}\\mathbf{b}$\n",
    "13. $\\mathbf{A}\\circ\\mathbf{A}$\n",
    "14. $\\mathbf{b}\\circ\\mathbf{c}$\n",
    "\n",
    "*Note: The element-wise (or Hadamard) product is the product of each element in one matrix with the corresponding element in another matrix, and is represented by the symbol \"$\\circ$\".*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7\n",
    "**[8 points]** **Eigenvectors and eigenvalues**. Eigenvectors and eigenvalues are useful for some machine learning algorithms, but the concepts take time to solidly grasp. They are used extensively in machine learning and in this course we will encounter them in relation to Principal Components Analysis (PCA), clustering algorithms, For an intuitive review of these concepts, explore this [interactive website at Setosa.io](http://setosa.io/ev/eigenvectors-and-eigenvalues/). Also, the series of linear algebra videos by Grant Sanderson of 3Brown1Blue are excellent and can be viewed on youtube [here](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab). For these questions, numpy may once again be helpful.\n",
    "\n",
    "1. Calculate the eigenvalues and corresponding eigenvectors of matrix $\\mathbf{A}$ above, from the last question.\n",
    "2. Choose one of the eigenvector/eigenvalue pairs, $\\mathbf{v}$ and $\\lambda$, and show that $\\mathbf{A} \\mathbf{v} = \\lambda \\mathbf{v}$. This relationship extends to higher orders: $\\mathbf{A} \\mathbf{A} \\mathbf{v} = \\lambda^2 \\mathbf{v}$\n",
    "3. Show that the eigenvectors are orthogonal to one another (e.g. their inner product is zero). This is true for eigenvectors from real, symmetric matrices. In three dimensions or less, this means that the eigenvectors are perpendicular to each other. Typically we use the orthogonal basis of our standard x, y, and z, Cartesian coordinates, which allows us, if we combine them linearly, to represent any point in a 3D space. But any three orthogonal vectors can do the same. We will see this property is used in PCA to identify the dimensions of greatest variation in our data when we discuss dimensionality reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Numerical Programming"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8\n",
    "**[10 points]** Loading data and gathering insights from a real dataset\n",
    "\n",
    "In data science, we often need to have a sense of the idiosyncrasies of the data, how they relate to the questions we are trying to answer, and to use that information to help us to determine what approach, such as machine learning, we may need to apply to achieve our goal. This exercise provides practice in exploring a dataset and answering question that might arise from applications related to the data.\n",
    "\n",
    "**Data**. The data for this problem can be found in the `data` subfolder in the `assignments` folder on [github](https://github.com/kylebradbury/ids705). The filename is `a1_egrid2016.xlsx`. This dataset is the Environmental Protection Agency's (EPA) [Emissions & Generation Resource Integrated Database (eGRID)](https://www.epa.gov/energy/emissions-generation-resource-integrated-database-egrid) containing information about all power plants in the United States, the amount of generation they produce, what fuel they use, the location of the plant, and many more quantities. We'll be using a subset of those data.\n",
    "\n",
    "The fields we'll be using include:\t\t\t\t\t\n",
    "    \n",
    "|field    |description|\n",
    "|:-----   |:-----|\n",
    "|SEQPLT16 |eGRID2016 Plant file sequence number (the index)| \n",
    "|PSTATABB |Plant state abbreviation|\n",
    "|PNAME    |Plant name |\n",
    "|LAT      |Plant latitude |\n",
    "|LON      |Plant longitude|\n",
    "|PLPRMFL  |Plant primary fuel |\n",
    "|CAPFAC   |Plant capacity factor |\n",
    "|NAMEPCAP |Plant nameplate capacity (Megawatts MW)|\n",
    "|PLNGENAN |Plant annual net generation (Megawatt-hours MWh)|\n",
    "|PLCO2EQA |Plant annual CO2 equivalent emissions (tons)|\n",
    "\n",
    "For more details on the data, you can refer to the [eGrid technical documents](https://www.epa.gov/sites/default/files/2021-02/documents/egrid2019_technical_guide.pdf). For example, you may want to review page 45 and the section \"Plant Primary Fuel (PLPRMFL)\", which gives the full names of the fuel types including WND for wind, NG for natural gas, BIT for Bituminous coal, etc.\n",
    "\n",
    "There also are a couple of \"gotchas\" to watch out for with this dataset:\n",
    "- The headers are on the second row and you'll want to ignore the first row (they're more detailed descriptions of the headers).\n",
    "- NaN values represent blanks in the data. These will appear regularly in real-world data, so getting experience working with these sorts of missing values will be important.\n",
    "\n",
    "**Your objective**. For this dataset, your goal is to answer the following questions about electricity generation in the United States:\n",
    "\n",
    "**(a)** Which plant has generated the most energy (measured in MWh)?\n",
    "\n",
    "**(b)** What is the name of the northern-most power plant in the United States?\n",
    "\n",
    "**(c)** What is the state where the northern-most power plant in the United States is located?\n",
    "\n",
    "**(d)** Plot a bar plot showing the amount of energy produced by each fuel type across all plants. \n",
    "\n",
    "**(e)** From the plot in (d), which fuel for generation produces the most energy (MWh) in the United States?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9\n",
    "**[6 points]** *Vectorization*. When we first learn to code and think about iterating over an array, we often use loops. If implemented correctly, that does the trick. In machine learning, we iterate over so much data that those loops can lead to significant slow downs if they are not computationally efficient. In Python, vectorizing code and relying on matrix operations with efficient tools like numpy is typically the faster approach. Of course, numpy relies on loops to complete the computation, but this is at a lower level of programming (typically in C), and therefore is much more efficient. This exercise will explore the benefits of vectorization. Since many machine learning techniques rely on matrix operations, it's helpful to begin thinking about implementing algorithms using vector forms.\n",
    "\n",
    "Begin by creating an array of 10 million random numbers using the numpy `random.randn` module. Compute the sum of the squares of those random numbers first in a for loop, then using Numpy's `dot` module to perform an inner (dot) product. Verify that your code produces the same output in each case. Time how long it takes to compute each and report the results and report the output. How many times faster is the vectorized code than the for loop approach? (Note - your results may vary from run to run).\n",
    "\n",
    "Your output should use the `print()` function as follows (where the # symbols represent your answers, to a reasonable precision of 4-5 significant figures):\n",
    "\n",
    "`Time [sec] (non-vectorized): ######`\n",
    "\n",
    "`Time [sec] (vectorized):     ######`\n",
    "\n",
    "`The vectorized code is ##### times faster than the nonvectorized code`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10\n",
    "**[10 points]** This exercise will walk through some basic numerical programming and probabilistic thinking exercises, two skills which are frequently used in machine learning for answering questions from our data.\n",
    "1. Synthesize $n=10^4$ normally distributed data points with mean $\\mu=2$ and a standard deviation of $\\sigma=1$. Call these observations from a random variable $X$, and call the vector of observations that you generate, $\\textbf{x}$.\n",
    "2. Calculate the mean and standard deviation of $\\textbf{x}$ to validate (1) and provide the result to a precision of four significant figures.\n",
    "3. Plot a histogram of the data in $\\textbf{x}$ with 30 bins\n",
    "4. What is the 90th percentile of $\\textbf{x}$? The 90th percentile is the value below which 90% of observations can be found.\n",
    "5. What is the 99th percentile of $\\textbf{x}$?\n",
    "6. Now synthesize $n=10^4$ normally distributed data points with mean $\\mu=0$ and a standard deviation of $\\sigma=3$. Call these observations from a random variable $Y$, and call the vector of observations that you generate, $\\textbf{y}$.\n",
    "7. Create a new figure and plot the histogram of the data in $\\textbf{y}$ on the same axes with the histogram of $\\textbf{x}$, so that both histograms can be seen and compared.\n",
    "8. Using the observations from $\\textbf{x}$ and $\\textbf{y}$, estimate $E[XY]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Version Control via Git"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11\n",
    "**[4 points]** Git is efficient for collaboration, and expectation in industry, and one of the best ways to share results in academia. You can even use some Git repositories (e.g. Github) as hosts for website, such as with the [course website](https://kylebradbury.github.io/ids705/index.html). As a data scientist with experience in machine learning, Git is expected. We will interact with Git repositories (a.k.a. repos) throughout this course, and your project will require the use of git repos for collaboration.\n",
    "\n",
    "Complete the [Atlassian Git tutorial](https://www.atlassian.com/git/tutorials/what-is-version-control), specifically the following listed sections. Try each concept that's presented. For this tutorial, instead of using BitBucket as your remote repository host, you may use your preferred platform such as [Github](https://github.com/) or [Duke's Gitlab](https://gitlab.oit.duke.edu/users/sign_in).\n",
    "1. [What is version control](https://www.atlassian.com/git/tutorials/what-is-version-control)\n",
    "2. [What is Git](https://www.atlassian.com/git/tutorials/what-is-git)\n",
    "3. [Install Git](https://www.atlassian.com/git/tutorials/install-git)\n",
    "4. [Setting up a repository](https://www.atlassian.com/git/tutorials/install-git)\n",
    "5. [Saving changes](https://www.atlassian.com/git/tutorials/saving-changes)\n",
    "6. [Inspecting a repository](https://www.atlassian.com/git/tutorials/inspecting-a-repository)\n",
    "7. [Undoing changes](https://www.atlassian.com/git/tutorials/undoing-changes)\n",
    "8. [Rewriting history](https://www.atlassian.com/git/tutorials/rewriting-history)\n",
    "9. [Syncing](https://www.atlassian.com/git/tutorials/syncing)\n",
    "10. [Making a pull request](https://www.atlassian.com/git/tutorials/making-a-pull-request)\n",
    "11. [Using branches](https://www.atlassian.com/git/tutorials/using-branches)\n",
    "12. [Comparing workflows](https://www.atlassian.com/git/tutorials/comparing-workflows)\n",
    "\n",
    "I also have created two videos on the topic to help you understand some of these concepts: [Git basics](https://www.youtube.com/watch?v=fBCwfoBr2ng) and a [step-by-step tutorial](https://www.youtube.com/watch?v=nH7qJHx-h5s).\n",
    "\n",
    "As an additional resource, Microsoft now offers a git [tutorial](https://learn.microsoft.com/en-us/collections/o1njfe825p602p) on this topic as well.\n",
    "\n",
    "For your answer, affirm that you *either* completed the tutorials above OR have previous experience with ALL of the concepts above. Confirm this by typing your name below and selecting the situation that applies from the two options in brackets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**\n",
    "\n",
    "> I, **Barbara Flores*, affirm that I have **completed the above tutorial***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Exploratory Data Analysis\n",
    "## 12\n",
    "**[15 points]** Here you'll bring together some of the individual skills that you demonstrated above and create a Jupyter notebook based blog post on your exploratory data analysis. Your goal is to identify a question or problem and to work towards solving it or providing additional information or evidence (data) related to it through your data analysis. Below, we walk through a process to follow for your analysis. Additionally, you can find an [example of a well-done exploratory data analysis here from past years](https://github.com/kylebradbury/ids705/blob/master/assignments/Assignment_1_Q12_Example.ipynb).\n",
    "\n",
    "1. Find a dataset that interests you and relates to a question or problem that you find intriguing.\n",
    "2. Describe the dataset, the source of the data, and the reason the dataset was of interest. Include a description of the features, data size, data creator and year of creation (if available), etc. What question are you hoping to answer through exploring the dataset?\n",
    "3. Check the data and see if they need to be cleaned: are there missing values? Are there clearly erroneous values? Do two tables need to be merged together? Clean the data so it can be visualized. If the data are clean, state how you know they are clean (what did you check?).\n",
    "3. Plot the data, demonstrating interesting features that you discover. Are there any relationships between variables that were surprising or patterns that emerged? Please exercise creativity and curiosity in your plots. You should have at least 4 plots exploring the data in different ways.\n",
    "4. What insights are you able to take away from exploring the data? Is there a reason why analyzing the dataset you chose is particularly interesting or important? Summarize this for a general audience (imagine your publishing a blog post online) - boil down your findings in a way that is accessible, but still accurate.\n",
    "\n",
    "Here your analysis will evaluated based on:\n",
    "1. Motivation: was the purpose of the choice of data clearly articulated? Why was the dataset chosen and what was the goal of the analysis?\n",
    "2. Data cleaning: were any issues with the data investigated and, if found, were they resolved?\n",
    "3. Quality of data exploration: were at least 4 unique plots (minimum) included and did those plots demonstrate interesting aspects of the data? Was there a clear purpose and takeaway from EACH plot? \n",
    "4. Interpretation: Were the insights revealed through the analysis and their potential implications clearly explained? Was there an overall conclusion to the analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">## <span style=\"color: #008080\">PAES: A New Exam Shaping Higher Education Access in Chile</span>\n",
    ">\n",
    ">In November 2022, Chile introduced a new selection test for access to higher education, the \"PAES: Prueba de Acceso a la EducaciÃ³n Superior\" (Higher Education Access Test). This test replaces the \"PSU: Prueba de SelecciÃ³n Universitaria\" (University Selection Test), which had been in use for almost 20 years.\n",
    ">\n",
    ">One of the objectives of the test change was to improve equity in access to higher education among students from private and public schools. The new test focuses more on understanding and applying knowledge, reducing the importance of memorization of facts and formulas. The hypothesis is that this change should particularly benefit students from public schools.\n",
    ">\n",
    ">However, the results of the second administration of the test were published on January 2, 2024, and the media has suggested that notable disparities persist among various types of establishments.\n",
    ">\n",
    ">We lack access to detailed data from the previous PSU test, and considering that both tests operate on distinct scales, conducting a thorough comparison becomes challenging within the confines of this examination. That's why, in this analysis, we will focus on understanding the performance of the current test. The goal is to grasp the differences that exist today in the results of the test that determines the university future of Chilean students. Therefore, the research question we aim to address in this analysis is:\n",
    ">\n",
    ">***How do the results of the new Higher Education Access Test (PAES) in Chile vary across different types of educational establishments and through various socioeconomic strata?***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">### <span style=\"color: #008080\">Database</span>\n",
    ">\n",
    ">This is why, in this Exploratory Data Analysis, I will examine the [database](https://datosabiertos.mineduc.cl/pruebas-de-admision-a-la-educacion-superior/) of the first PAES exam results (Ministry of Education of Chile, 2024) to identify the differences among different students and try to better understand the situation of access to higher education in Chile.\n",
    ">\n",
    ">In particular, we will use 2 files: \"Prueba de Acceso a la Educacion Superior 2023 â€“ Inscritos Puntajes\" and \"Prueba de Acceso a la EducaciÃ³n Superior 2023 â€“ SocioeconÃ³micos\"\n",
    ">These files contain disaggregated information at the student level, with an anonymized identifier for each person that allows connecting both databases.\n",
    ">\n",
    ">Both databases have 296,812 student records, with 117 and 39 columns, respectively. They each have a unique student identifier, making it possible to merge them together.\n",
    ">\n",
    ">These databases include information such as the school name from which the student comes, region and city, type of educational establishment (private, public, or co-financed), gender, high school GPA, scores in different selection tests, number of correct, incorrect, and omitted questions in each test type, etc. They also include information on the family's socioeconomic stratum, grouped by deciles.\n",
    ">\n",
    ">For the present analysis, we will use the following variables. However, the description of the 39 variables can be found on the [database page](https://datosabiertos.mineduc.cl/pruebas-de-admision-a-la-educacion-superior/).\n",
    ">\n",
    ">\n",
    ">| Name                       | Type     | Description                                                                       | Values                                                                                                                  |\n",
    ">|----------------------------- |----------|-----------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------|\n",
    ">| DEPENDENCIA                  | Numeric  | Dependency group.                                                                | 1: Municipal Corporation, 2: Municipal, 3: Subsidized Private, 4: Non-subsidized Private, 5: Delegated Administration Corporation, 6: Local Education Service (SLE)   |\n",
    ">| MATE1_REG_ACTUAL             | Numeric  | Score in Mathematics Competency Test 1 in the current session                     | -                                                                                                                       |\n",
    ">| CLEC_REG_ACTUAL              | Numeric  | Score in Reading Competency Test in the current regular session                    | -                                                                                                                       |\n",
    ">| INGRESO_PERCAPITA_GRUPO_FA   | Numeric  | What is the per capita income of your family group?                                 | 1: 1st decile, 2: 2nd decile: and so on... and 99: Prefer not to respond                                               |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">### <span style=\"color: #008080\">Data Loading and Cleaning</span>\n",
    "> \n",
    ">First, we merge the data, and then we remove null values. For the purposes of the analyses we want to perform, we will remove records with null values in the columns of dependency type, high school GPA, general mathematics test, and reading comprehension test. At the moment, we will not remove the values of 99 in the variable 'per capita income' (Prefer not to respond) since they represent a larger proportion of the database.\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_test_score = \"Prueba-de-Acceso-a-la-Educacion-Superior-2023-Inscritos-Puntajes/A_INSCRITOS_PUNTAJES_2023_PAES_PUB_MRUN.csv\"\n",
    "test_score = pd.read_csv(path_test_score, delimiter=\";\", low_memory=False)\n",
    "test_score.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_socioeconomic = \"Prueba-de-Acceso-a-la-Educacion-Superior-2023-Socioeconomicos/B_SOCIOECONOMICO_DOMICILIO_2023_PAES_PUB_MRUN.csv\"\n",
    "socioeconomic = pd.read_csv(path_socioeconomic, delimiter=\";\")\n",
    "socioeconomic.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(test_score, socioeconomic, how=\"inner\", on=\"MRUN\")\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data[\"MATE1_REG_ACTUAL\"] != ' ']\n",
    "data = data[data[\"CLEC_REG_ACTUAL\"] != ' ']\n",
    "data = data[data[\"DEPENDENCIA\"] != ' ']\n",
    "data = data[data[\"DEPENDENCIA\"] != ' ']\n",
    "\n",
    "data[\"MATE1_REG_ACTUAL\"] = data[\"MATE1_REG_ACTUAL\"].astype(int)\n",
    "data[\"CLEC_REG_ACTUAL\"] = data[\"CLEC_REG_ACTUAL\"].astype(int)\n",
    "\n",
    "data = data[data[\"MATE1_REG_ACTUAL\"] > 0]\n",
    "data = data[data[\"CLEC_REG_ACTUAL\"] > 0]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">After removing null values for our target variables, our dataset goes from 296,812 to 229,431 records. We validate that our variables do not have null values (except for the value 99 for per capita income)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of the dataset:\", data.shape)\n",
    "print(\"Range of MATE1_REG_ACTUAL:\", min(\n",
    "    data[\"MATE1_REG_ACTUAL\"]), \"-\", max(data[\"MATE1_REG_ACTUAL\"]))\n",
    "print(\"Range of CLEC_REG_ACTUAL:\", min(\n",
    "    data[\"CLEC_REG_ACTUAL\"]), \"-\", max(data[\"CLEC_REG_ACTUAL\"]))\n",
    "print(\"\\nValue counts for DEPENDENCIA:\")\n",
    "print(data[\"DEPENDENCIA\"].value_counts().sort_index())\n",
    "print(\"\\nPercentile of Per capita income:\")\n",
    "print(data[\"INGRESO_PERCAPITA_GRUPO_FA\"].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">### <span style=\"color: #008080\">EDA</span>\n",
    ">\n",
    ">Once our dataset is clean, we proceed to analyze its numbers. Firstly, we want to understand the distribution of PAES scores in the two main tests we will be investigating (Math and Reading), by type of educational establishment. Therefore, we will initially focus on understanding what proportion of students taking these tests belong to each type of establishment.\n",
    ">\n",
    ">We observe that the majority of Chilean students (55%) attend private schools with state subsidy. These schools receive funding from both the government and the families of the students. Then, 30% study in schools fully funded by the government, and 12% attend private schools. There are also Delegated Administration schools, which are owned by the state but managed and financed by private corporations. However, as they represent a special case and a minority, we will not analyze these schools in greater detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependency_mapping = {\n",
    "    \"1\": \"Public School\",\n",
    "    \"2\": \"Public School\",\n",
    "    \"3\": \"Private subsidized School\",\n",
    "    \"4\": \"Private non-subsidized School\",\n",
    "    \"5\": \"Delegated Administration School\",\n",
    "    \"6\": \"Public School\",\n",
    "}\n",
    "\n",
    "dependency_order = [\n",
    "    \"Delegated Administration School\",\n",
    "    \"Private non-subsidized School\",\n",
    "    \"Private subsidized School\",\n",
    "    \"Public School\",\n",
    "]\n",
    "\n",
    "data[\"DEPENDENCY_NAME\"] = data[\"DEPENDENCIA\"].replace(dependency_mapping)\n",
    "dependency_counts = data[\"DEPENDENCY_NAME\"].value_counts()\n",
    "dependency_counts = dependency_counts.reindex(dependency_order)\n",
    "\n",
    "percentage_labels = (dependency_counts / len(data)) * 100\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bar_plot = dependency_counts.plot(kind=\"barh\", color=\"skyblue\")\n",
    "\n",
    "for index, value in enumerate(dependency_counts):\n",
    "    percentage_label = f\"{percentage_labels.iloc[index]:.1f}%\"\n",
    "    bar_plot.text(\n",
    "        value, index, percentage_label, ha=\"center\", va=\"center\", color=\"black\"\n",
    "    )\n",
    "\n",
    "plt.title(\"Number of Enrolled Students by Dependency for PAES Test in Chile 2023\")\n",
    "plt.xlabel(\"Number of Students\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "# MATH TEST\n",
    "sns.boxplot(ax=axes[0], x=\"DEPENDENCY_NAME\", y=\"MATE1_REG_ACTUAL\",\n",
    "            data=data, order=dependency_order[::-1], palette=\"Set3\")\n",
    "\n",
    "axes[0].set_title(\"Math PAES Test Scores by Dependency in Chile 2023\")\n",
    "axes[0].set_xlabel(\"\")\n",
    "axes[0].set_ylabel(\"Scores in Mathematical Competence Test 1\")\n",
    "axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# READING TEST\n",
    "sns.boxplot(ax=axes[1], x=\"DEPENDENCY_NAME\", y=\"CLEC_REG_ACTUAL\",\n",
    "            data=data, order=dependency_order[::-1], palette=\"Set3\")\n",
    "\n",
    "axes[1].set_title(\"Reading PAES Test Scores by Dependency in Chile 2023\")\n",
    "axes[1].set_xlabel(\"\")\n",
    "axes[1].set_ylabel(\"Scores in Reading Comprehension Test\")\n",
    "axes[1].set_xticklabels(axes[0].get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "\n",
    "plt.subplots_adjust(wspace=0.3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">ewe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data[data[\"INGRESO_PERCAPITA_GRUPO_FA\"] != 99]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "\n",
    "# MATH TEST\n",
    "sns.boxplot(ax=axes[0], x=\"INGRESO_PERCAPITA_GRUPO_FA\", y=\"MATE1_REG_ACTUAL\", data=data2,\n",
    "            order=data2[\"INGRESO_PERCAPITA_GRUPO_FA\"].sort_values().unique(),\n",
    "            palette=\"Set3\")\n",
    "\n",
    "axes[0].set_title(\n",
    "    \"Math PAES Test Scores by Family Income Decile in Chile 2023\")\n",
    "axes[0].set_xlabel(\"\")\n",
    "axes[0].set_ylabel(\"Scores in Mathematical Competence Test 1\")\n",
    "axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# READING TEST\n",
    "sns.boxplot(ax=axes[1], x=\"INGRESO_PERCAPITA_GRUPO_FA\", y=\"CLEC_REG_ACTUAL\", data=data2,\n",
    "            order=data2[\"INGRESO_PERCAPITA_GRUPO_FA\"].sort_values().unique(),\n",
    "            palette=\"Set3\")\n",
    "\n",
    "axes[1].set_title(\n",
    "    \"Reading PAES Test Scores by Family Income Decile in Chile 2023\")\n",
    "axes[1].set_xlabel(\"\")\n",
    "axes[1].set_ylabel(\"Scores in Reading Comprehension Test\")\n",
    "axes[1].set_xticklabels(axes[0].get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "plt.subplots_adjust(wspace=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">### <span style=\"color: #008080\">Conclusions and Future Research</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">### <span style=\"color: #008080\">References</span>\n",
    ">\n",
    "> Education in Chile https://en.wikipedia.org/wiki/Education_in_Chile\n",
    ">\n",
    "> Database: https://datosabiertos.mineduc.cl/pruebas-de-admision-a-la-educacion-superior/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "nteract": {
   "version": "0.28.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "643px",
    "left": "1548px",
    "right": "20px",
    "top": "121px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "f54e0b9f92ceb50229b3b7ea26b5d02f05ce8845dd8d997c014a0efede4c9c20"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
